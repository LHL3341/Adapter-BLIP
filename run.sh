nohup python -u -m torch.distributed.run --nproc_per_node 8 main.py > pretrain.log 2>&1 &